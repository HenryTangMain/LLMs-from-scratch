# Chapter 2: Working with Text Data - Call Flow Diagram

This document provides a detailed call flow diagram for the data preprocessing pipeline in Chapter 2.

## Overview

Chapter 2 implements the complete data pipeline that transforms raw text into embedded tensors ready for LLM training. The pipeline consists of:

1. **Tokenization** - Converting text to token IDs
2. **Dataset Creation** - Creating input/target pairs with sliding windows
3. **Batching** - Grouping samples into batches
4. **Embedding** - Converting token IDs to continuous vectors
5. **Position Encoding** - Adding positional information

---

## Complete Data Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CH02: WORKING WITH TEXT DATA                         â”‚
â”‚                        Data Pipeline Flow                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. RAW TEXT INPUT
   â”‚
   â”œâ”€â–º "the-verdict.txt" (or any text file)
   â”‚   â””â”€â–º Read file content as string
   â”‚
   â””â”€â–º raw_text = "I HAD always thought Jack Gisburn..."


2. TOKENIZATION (Text â†’ Token IDs)
   â”‚
   â”œâ”€â–º tiktoken.get_encoding("gpt2")
   â”‚   â””â”€â–º BPE Tokenizer initialized
   â”‚
   â”œâ”€â–º tokenizer.encode(text, allowed_special={"<|endoftext|>"})
   â”‚   â”‚
   â”‚   â”œâ”€â–º Breaks text into subwords/characters
   â”‚   â”œâ”€â–º Maps each token to integer ID
   â”‚   â””â”€â–º Returns: [40, 367, 2885, 1464, 1807, ...]
   â”‚
   â””â”€â–º token_ids (List of integers)


3. DATASET CREATION (Sliding Window)
   â”‚
   â”œâ”€â–º GPTDatasetV1(txt, tokenizer, max_length, stride)
   â”‚   â”‚
   â”‚   â”œâ”€â–º __init__(txt, tokenizer, max_length, stride):
   â”‚   â”‚   â”‚
   â”‚   â”‚   â”œâ”€â–º Encode entire text to token_ids
   â”‚   â”‚   â”‚
   â”‚   â”‚   â”œâ”€â–º Sliding window loop:
   â”‚   â”‚   â”‚   for i in range(0, len(token_ids)-max_length, stride):
   â”‚   â”‚   â”‚       â”‚
   â”‚   â”‚   â”‚       â”œâ”€â–º input_chunk = token_ids[i : i+max_length]
   â”‚   â”‚   â”‚       â”‚   â””â”€â–º [290, 4920, 2241, 287]
   â”‚   â”‚   â”‚       â”‚
   â”‚   â”‚   â”‚       â””â”€â–º target_chunk = token_ids[i+1 : i+max_length+1]
   â”‚   â”‚   â”‚           â””â”€â–º [4920, 2241, 287, 257]  (shifted by 1)
   â”‚   â”‚   â”‚
   â”‚   â”‚   â””â”€â–º Store as tensors in self.input_ids & self.target_ids
   â”‚   â”‚
   â”‚   â”œâ”€â–º __len__(): Returns number of samples
   â”‚   â”‚
   â”‚   â””â”€â–º __getitem__(idx): Returns (input_ids[idx], target_ids[idx])
   â”‚
   â””â”€â–º Dataset object ready


4. DATALOADER CREATION (Batching)
   â”‚
   â”œâ”€â–º create_dataloader_v1(txt, batch_size, max_length, stride, ...)
   â”‚   â”‚
   â”‚   â”œâ”€â–º Initialize tokenizer
   â”‚   â”‚
   â”‚   â”œâ”€â–º Create GPTDatasetV1 (from step 3)
   â”‚   â”‚
   â”‚   â””â”€â–º DataLoader(dataset, batch_size, shuffle, drop_last, ...)
   â”‚       â”‚
   â”‚       â””â”€â–º Returns batched samples
   â”‚           â”œâ”€â–º inputs:  [batch_size, max_length]
   â”‚           â””â”€â–º targets: [batch_size, max_length]
   â”‚
   â””â”€â–º DataLoader object (iterable)


5. ITERATION & BATCHING
   â”‚
   â”œâ”€â–º for batch in dataloader:
   â”‚       x, y = batch
   â”‚
   â”‚   Example with batch_size=8, max_length=4:
   â”‚       x shape: torch.Size([8, 4])  â† 8 samples, 4 tokens each
   â”‚       y shape: torch.Size([8, 4])  â† targets (shifted by 1)
   â”‚
   â””â”€â–º Batched token IDs ready for embedding


6. TOKEN EMBEDDING (IDs â†’ Vectors)
   â”‚
   â”œâ”€â–º token_embedding_layer = nn.Embedding(vocab_size, output_dim)
   â”‚   â”‚                                    (50257,      256)
   â”‚   â”‚
   â”‚   â””â”€â–º Embedding matrix: [50257 Ã— 256]
   â”‚       Each token ID maps to a 256-dim vector
   â”‚
   â”œâ”€â–º token_embeddings = token_embedding_layer(x)
   â”‚   â”‚
   â”‚   â””â”€â–º Input:  [8, 4]      (batch_size, max_length)
   â”‚       Output: [8, 4, 256] (batch_size, max_length, embedding_dim)
   â”‚
   â””â”€â–º Token vectors created


7. POSITIONAL EMBEDDING (Position â†’ Vectors)
   â”‚
   â”œâ”€â–º pos_embedding_layer = nn.Embedding(context_length, output_dim)
   â”‚   â”‚                                  (1024,          256)
   â”‚   â”‚
   â”‚   â””â”€â–º Position embedding matrix: [1024 Ã— 256]
   â”‚       Each position (0-1023) has a 256-dim vector
   â”‚
   â”œâ”€â–º pos_embeddings = pos_embedding_layer(torch.arange(max_length))
   â”‚   â”‚                                    [0, 1, 2, 3]
   â”‚   â”‚
   â”‚   â””â”€â–º Input:  [4]         (max_length)
   â”‚       Output: [4, 256]    (max_length, embedding_dim)
   â”‚
   â””â”€â–º Position vectors created


8. FINAL INPUT EMBEDDINGS (Combine Token + Position)
   â”‚
   â”œâ”€â–º input_embeddings = token_embeddings + pos_embeddings
   â”‚   â”‚
   â”‚   â”‚   token_embeddings: [8, 4, 256]
   â”‚   â”‚   pos_embeddings:   [4, 256]     (broadcasted to [8, 4, 256])
   â”‚   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚   â”‚   result:           [8, 4, 256]
   â”‚   â”‚
   â”‚   â””â”€â–º Each token now has:
   â”‚       â”œâ”€â–º Semantic information (from token embedding)
   â”‚       â””â”€â–º Position information (from positional embedding)
   â”‚
   â””â”€â–º READY FOR LLM INPUT! âœ“
```

---

## Key Components

### GPTDatasetV1 Class

A PyTorch Dataset that implements the sliding window approach for creating training samples.

**Methods:**
- `__init__(txt, tokenizer, max_length, stride)`: Create input/target pairs with sliding window
- `__len__()`: Return number of samples
- `__getitem__(idx)`: Get sample by index

**Sliding Window Logic:**
```
Input sequence:  [A, B, C, D, E, F, G, H, I, J]
max_length = 4, stride = 4

Window 1:
  Input:  [A, B, C, D]
  Target: [B, C, D, E]

Window 2 (stride=4, starts at position 4):
  Input:  [E, F, G, H]
  Target: [F, G, H, I]
```

### create_dataloader_v1() Function

Convenience function that encapsulates the entire preprocessing pipeline.

**Parameters:**
- `txt`: Raw text string
- `batch_size`: Number of samples per batch
- `max_length`: Sequence length (context window)
- `stride`: Step size for sliding window
- `shuffle`: Whether to shuffle data
- `drop_last`: Drop incomplete final batch
- `num_workers`: Number of parallel workers

**Returns:** PyTorch DataLoader object

---

## Detailed Function Call Sequence

```python
# ============================================
# STEP 1: Load Raw Text
# ============================================
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
# Result: String containing full text


# ============================================
# STEP 2: Create DataLoader (encapsulates preprocessing)
# ============================================
dataloader = create_dataloader_v1(
    txt=raw_text,
    batch_size=8,
    max_length=4,
    stride=4,
    shuffle=True
)

# Internal execution flow:
#
# 2a. Initialize tokenizer
tokenizer = tiktoken.get_encoding("gpt2")
#
# 2b. Create dataset with sliding window
dataset = GPTDatasetV1(raw_text, tokenizer, max_length=4, stride=4)
#     â”‚
#     â”œâ”€â–º token_ids = tokenizer.encode(raw_text, allowed_special={"<|endoftext|>"})
#     â”‚   # Example: [40, 367, 2885, 1464, 1807, 3619, 402, 271, ...]
#     â”‚
#     â””â”€â–º for i in range(0, len(token_ids) - max_length, stride):
#             input_chunk = token_ids[i : i + max_length]
#             target_chunk = token_ids[i + 1 : i + max_length + 1]
#             self.input_ids.append(torch.tensor(input_chunk))
#             self.target_ids.append(torch.tensor(target_chunk))
#
# 2c. Wrap in DataLoader for batching
return DataLoader(
    dataset,
    batch_size=8,
    shuffle=True,
    drop_last=True,
    num_workers=0
)


# ============================================
# STEP 3: Create Embedding Layers
# ============================================
vocab_size = 50257      # GPT-2 vocabulary size
output_dim = 256        # Embedding dimension
context_length = 1024   # Maximum sequence length

token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)


# ============================================
# STEP 4: Iterate and Embed
# ============================================
for batch in dataloader:
    x, y = batch
    # x: torch.Size([8, 4])  - 8 samples, 4 tokens each
    # y: torch.Size([8, 4])  - targets (shifted by 1 position)

    # 4a. Convert token IDs to vectors
    token_embeddings = token_embedding_layer(x)
    # Input:  [8, 4]      (batch_size, max_length)
    # Output: [8, 4, 256] (batch_size, max_length, embedding_dim)

    # 4b. Get positional embeddings
    pos_embeddings = pos_embedding_layer(torch.arange(max_length))
    # Input:  [4]      (sequence positions: 0, 1, 2, 3)
    # Output: [4, 256] (max_length, embedding_dim)

    # 4c. Combine token + position embeddings
    input_embeddings = token_embeddings + pos_embeddings
    # token_embeddings: [8, 4, 256]
    # pos_embeddings:   [4, 256]     <- broadcasted to [8, 4, 256]
    # result:           [8, 4, 256]

    # âœ“ Ready for Transformer model!
    break  # Process first batch only for this example
```

---

## Visual Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA TRANSFORMATION                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Raw Text String
"I HAD always thought Jack Gisburn rather a cheap genius..."
      â†“
[Tokenization: tiktoken BPE]
      â†“
Token IDs (integers)
[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, ...]
      â†“
[Sliding Window: GPTDatasetV1]
Creates overlapping input/target pairs
      â†“
Dataset Samples
Sample 0: input=[40, 367, 2885, 1464], target=[367, 2885, 1464, 1807]
Sample 1: input=[1807, 3619, 402, 271], target=[3619, 402, 271, 10899]
Sample 2: input=[10899, 2138, 257, 7026], target=[2138, 257, 7026, 15632]
...
      â†“
[Batching: PyTorch DataLoader]
Groups samples into batches
      â†“
Batched Tensors
x: torch.Size([8, 4])  - batch of 8 sequences, 4 tokens each
y: torch.Size([8, 4])  - corresponding targets
      â†“
[Token Embedding Layer]
Maps each token ID to a learned vector
      â†“
Token Embeddings
torch.Size([8, 4, 256])  - each token â†’ 256-dim vector
      â†“
[+ Positional Embedding Layer]
Adds position-specific information
      â†“
Final Input Embeddings
torch.Size([8, 4, 256])  - token info + position info
      â†“
Ready for Transformer! ğŸš€
```

---

## Token ID to Embedding Lookup Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            HOW TOKEN EMBEDDING WORKS                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Given:
  vocab_size = 50257
  output_dim = 256

Embedding Layer:
  token_embedding_layer = nn.Embedding(50257, 256)

  Creates a matrix of shape [50257, 256]:

  Token ID â”‚ Embedding Vector (256 dimensions)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     0     â”‚ [0.123, -0.456, 0.789, ..., 0.234]
     1     â”‚ [-0.234, 0.567, -0.890, ..., 0.345]
     2     â”‚ [0.345, -0.678, 0.901, ..., -0.456]
    ...    â”‚ ...
    40     â”‚ [0.456, 0.789, -0.123, ..., 0.567]
    ...    â”‚ ...
   50256   â”‚ [-0.567, 0.890, 0.234, ..., -0.678]

Lookup Process:
  Input token IDs: [40, 367, 2885, 1464]

  Each ID fetches its corresponding row:
  ID 40    â†’ embedding_layer.weight[40]    = [0.456, 0.789, ...]
  ID 367   â†’ embedding_layer.weight[367]   = [0.234, -0.123, ...]
  ID 2885  â†’ embedding_layer.weight[2885]  = [-0.890, 0.456, ...]
  ID 1464  â†’ embedding_layer.weight[1464]  = [0.678, -0.234, ...]

  Result: tensor of shape [4, 256]
```

---

## Positional Embedding Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          WHY WE NEED POSITIONAL EMBEDDINGS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem:
  Without position info, "cat ate mouse" and "mouse ate cat"
  would have identical embeddings (just different order).

Solution:
  Add position-specific vectors to each token embedding.

Example:
  Sentence: "The cat sat"
  Tokens:   ["The", "cat", "sat"]
  Token IDs: [464, 3797, 3332]

  Position embeddings for positions [0, 1, 2]:

  Position 0 â†’ [0.123, -0.456, 0.789, ..., 0.234]
  Position 1 â†’ [-0.234, 0.567, -0.890, ..., 0.345]
  Position 2 â†’ [0.345, -0.678, 0.901, ..., -0.456]

  Final embeddings = token_embeddings + pos_embeddings

  "The" at position 0:
    token_emb[464] + pos_emb[0] = combined vector

  "cat" at position 1:
    token_emb[3797] + pos_emb[1] = combined vector

  "sat" at position 2:
    token_emb[3332] + pos_emb[2] = combined vector
```

---

## Tensor Shape Transformations Summary

| Stage | Input Shape | Output Shape | Description |
|-------|-------------|--------------|-------------|
| Raw Text | String | - | "I HAD always thought..." |
| Tokenization | String | `[seq_len]` | `[40, 367, 2885, ...]` |
| Dataset Creation | `[seq_len]` | Multiple `[max_length]` pairs | Sliding window chunks |
| DataLoader Batching | `[max_length]` | `[batch_size, max_length]` | `[8, 4]` |
| Token Embedding | `[8, 4]` | `[8, 4, 256]` | Each ID â†’ 256-dim vector |
| Positional Embedding | `[4]` | `[4, 256]` | Position â†’ 256-dim vector |
| Final Embedding | `[8, 4, 256]` + `[4, 256]` | `[8, 4, 256]` | Broadcasting addition |

---

## Key Hyperparameters

```python
# Text preprocessing
max_length = 4        # Context window size (tokens per sample)
stride = 4            # Sliding window step size
batch_size = 8        # Samples per batch

# Embedding dimensions
vocab_size = 50257    # GPT-2 BPE vocabulary size
output_dim = 256      # Embedding vector dimension
context_length = 1024 # Maximum sequence length

# DataLoader settings
shuffle = True        # Randomize sample order
drop_last = True      # Drop incomplete final batch
num_workers = 0       # Number of parallel data loading workers
```

---

## Important Notes

1. **Targets are shifted inputs**: Target sequence is input shifted by 1 position to the right. This enables next-token prediction training.

2. **Sliding window overlap**: When `stride < max_length`, windows overlap, providing more training samples but potentially causing overfitting.

3. **Broadcasting in embedding addition**: When adding `[8, 4, 256]` + `[4, 256]`, PyTorch automatically broadcasts the second tensor across the batch dimension.

4. **Byte Pair Encoding (BPE)**: GPT-2 uses BPE tokenization which breaks unknown words into subword units, eliminating the need for `<UNK>` tokens.

5. **Special tokens**: GPT-2 uses `<|endoftext|>` to mark boundaries between different text sources and for padding.

---

## Code Location

- **Main notebook**: `ch02/01_main-chapter-code/ch02.ipynb`
- **Condensed version**: `ch02/01_main-chapter-code/dataloader.ipynb`
- **Exercises**: `ch02/01_main-chapter-code/exercise-solutions.ipynb`

---

## Next Steps

After completing Chapter 2, you'll have:
- âœ… Tokenized text using BPE
- âœ… Created training datasets with sliding windows
- âœ… Built PyTorch DataLoaders for batching
- âœ… Embedded tokens in continuous vector space
- âœ… Added positional information to embeddings

**Ready for Chapter 3**: Implementing attention mechanisms! ğŸš€
