# Chapter 3: Coding Attention Mechanisms - Call Flow Diagram

This document provides a detailed call flow diagram for the attention mechanism implementation in Chapter 3.

## Overview

Chapter 3 implements the attention mechanism, which is the core innovation that enables LLMs to process context. This chapter builds on Chapter 2's data pipeline and implements:

1. **Self-Attention** - Computing attention scores and context vectors
2. **Causal Attention** - Adding masking for autoregressive models
3. **Multi-Head Attention** - Parallel attention computations
4. **Scaled Dot-Product Attention** - Normalizing attention scores

---

## Complete Attention Mechanism Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CH03: CODING ATTENTION MECHANISMS                      â”‚
â”‚                     Attention Computation Flow                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: Token Embeddings from Ch02
  Shape: [batch_size, num_tokens, d_in]
  Example: [8, 4, 256]
  â”‚
  â”‚   Each token has a 256-dimensional embedding vector
  â”‚   containing semantic and positional information
  â”‚
  â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: LINEAR PROJECTIONS (Query, Key, Value)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º W_query = Linear(d_in, d_out)
  â”‚   queries = W_query(x)
  â”‚   â”‚
  â”‚   â””â”€â–º Shape: [batch, num_tokens, d_out]
  â”‚       Example: [8, 4, 256]
  â”‚
  â”œâ”€â–º W_key = Linear(d_in, d_out)
  â”‚   keys = W_key(x)
  â”‚   â”‚
  â”‚   â””â”€â–º Shape: [batch, num_tokens, d_out]
  â”‚       Example: [8, 4, 256]
  â”‚
  â””â”€â–º W_value = Linear(d_in, d_out)
      values = W_value(x)
      â”‚
      â””â”€â–º Shape: [batch, num_tokens, d_out]
          Example: [8, 4, 256]

  Q, K, V: Three different "views" of the same input
  - Query: "What am I looking for?"
  - Key:   "What do I contain?"
  - Value: "What information do I have?"


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: COMPUTE ATTENTION SCORES (Dot Product)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º attn_scores = queries @ keys.transpose(-2, -1)
  â”‚   â”‚
  â”‚   â”‚   queries: [batch, num_tokens, d_out]
  â”‚   â”‚   keys.T:  [batch, d_out, num_tokens]
  â”‚   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚   â”‚   result:  [batch, num_tokens, num_tokens]
  â”‚   â”‚
  â”‚   â””â”€â–º Shape: [8, 4, 4]
  â”‚       â”‚
  â”‚       â”‚   Each position computes similarity with all positions:
  â”‚       â”‚
  â”‚       â”‚        Token0  Token1  Token2  Token3
  â”‚       â”‚   Token0 [s00    s01    s02    s03]
  â”‚       â”‚   Token1 [s10    s11    s12    s13]
  â”‚       â”‚   Token2 [s20    s21    s22    s23]
  â”‚       â”‚   Token3 [s30    s31    s32    s33]
  â”‚       â”‚
  â”‚       â””â”€â–º sij = similarity between token i and token j
  â”‚
  â””â”€â–º Higher scores = more relevant context


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: APPLY CAUSAL MASK (for autoregressive models)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º Create upper triangular mask:
  â”‚   mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1)
  â”‚   â”‚
  â”‚   â”‚   Example for 4 tokens:
  â”‚   â”‚        0  1  2  3
  â”‚   â”‚   0 [  0  1  1  1 ]  â† Token 0 can't see future tokens
  â”‚   â”‚   1 [  0  0  1  1 ]  â† Token 1 can't see tokens 2,3
  â”‚   â”‚   2 [  0  0  0  1 ]  â† Token 2 can't see token 3
  â”‚   â”‚   3 [  0  0  0  0 ]  â† Token 3 sees all previous
  â”‚   â”‚
  â”‚   â””â”€â–º 1 = mask (hide), 0 = keep (show)
  â”‚
  â”œâ”€â–º Apply mask to attention scores:
  â”‚   attn_scores.masked_fill_(mask.bool(), -torch.inf)
  â”‚   â”‚
  â”‚   â”‚   Before masking:
  â”‚   â”‚        Token0  Token1  Token2  Token3
  â”‚   â”‚   Token0 [0.5    0.7    0.3    0.9]
  â”‚   â”‚   Token1 [0.4    0.6    0.8    0.2]
  â”‚   â”‚   Token2 [0.3    0.5    0.4    0.7]
  â”‚   â”‚   Token3 [0.6    0.4    0.5    0.8]
  â”‚   â”‚
  â”‚   â”‚   After masking (future tokens = -inf):
  â”‚   â”‚        Token0  Token1  Token2  Token3
  â”‚   â”‚   Token0 [0.5    -inf   -inf   -inf]
  â”‚   â”‚   Token1 [0.4    0.6    -inf   -inf]
  â”‚   â”‚   Token2 [0.3    0.5    0.4    -inf]
  â”‚   â”‚   Token3 [0.6    0.4    0.5    0.8]
  â”‚   â”‚
  â”‚   â””â”€â–º Prevents token from attending to future positions
  â”‚
  â””â”€â–º WHY? For next-token prediction, token N shouldn't see N+1!


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: SCALE ATTENTION SCORES                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º scaled_scores = attn_scores / sqrt(d_out)
  â”‚   â”‚
  â”‚   â””â”€â–º WHY? Prevent extremely large values before softmax
  â”‚       - Large dot products â†’ extreme softmax outputs
  â”‚       - Scaling stabilizes training
  â”‚       - sqrt(d_out) is theoretically motivated
  â”‚
  â””â”€â–º Shape unchanged: [batch, num_tokens, num_tokens]


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: SOFTMAX (Convert scores to probabilities)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º attn_weights = softmax(scaled_scores, dim=-1)
  â”‚   â”‚
  â”‚   â”‚   Converts each row to probability distribution:
  â”‚   â”‚
  â”‚   â”‚   After softmax:
  â”‚   â”‚        Token0  Token1  Token2  Token3
  â”‚   â”‚   Token0 [1.0    0.0    0.0    0.0]  â† Only sees itself
  â”‚   â”‚   Token1 [0.3    0.7    0.0    0.0]  â† Mostly focuses on Token1
  â”‚   â”‚   Token2 [0.2    0.3    0.5    0.0]  â† Balanced attention
  â”‚   â”‚   Token3 [0.1    0.2    0.3    0.4]  â† Attends to all
  â”‚   â”‚
  â”‚   â”‚   Properties:
  â”‚   â”‚   - Each row sums to 1.0
  â”‚   â”‚   - All values between 0 and 1
  â”‚   â”‚   - Represents attention distribution
  â”‚   â”‚
  â”‚   â””â”€â–º Shape: [batch, num_tokens, num_tokens]
  â”‚
  â”œâ”€â–º Optional: Apply dropout for regularization
  â”‚   attn_weights = dropout(attn_weights)
  â”‚
  â””â”€â–º Attention weights ready for value aggregation


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: COMPUTE CONTEXT VECTORS (Weighted sum of values)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º context_vec = attn_weights @ values
  â”‚   â”‚
  â”‚   â”‚   attn_weights: [batch, num_tokens, num_tokens]
  â”‚   â”‚   values:       [batch, num_tokens, d_out]
  â”‚   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚   â”‚   result:       [batch, num_tokens, d_out]
  â”‚   â”‚
  â”‚   â””â”€â–º Shape: [8, 4, 256]
  â”‚
  â”œâ”€â–º Each position gets weighted combination of all values:
  â”‚   â”‚
  â”‚   â”‚   For token i:
  â”‚   â”‚   context[i] = Î£(attention_weight[i,j] * value[j])
  â”‚   â”‚
  â”‚   â”‚   Example for token 2:
  â”‚   â”‚   context[2] = 0.2*value[0] + 0.3*value[1] + 0.5*value[2]
  â”‚   â”‚
  â”‚   â””â”€â–º Higher attention weight = more contribution
  â”‚
  â””â”€â–º OUTPUT: Context-aware representations!

  Each token now contains information from tokens it attended to
```

---

## Multi-Head Attention Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MULTI-HEAD ATTENTION (Parallel Processing)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: [batch, num_tokens, d_in]  (e.g., [8, 4, 256])
  â”‚
  â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Project to Q, K, V                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º W_query(x) â†’ [batch, num_tokens, d_out]
  â”œâ”€â–º W_key(x)   â†’ [batch, num_tokens, d_out]
  â””â”€â–º W_value(x) â†’ [batch, num_tokens, d_out]


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Reshape for Multiple Heads                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚   Given: num_heads = 12, d_out = 768
  â”‚   Then:  head_dim = d_out / num_heads = 64
  â”‚
  â”œâ”€â–º Reshape Q, K, V:
  â”‚   [batch, num_tokens, d_out]
  â”‚   â†’ [batch, num_tokens, num_heads, head_dim]
  â”‚   â†’ [batch, num_heads, num_tokens, head_dim]
  â”‚   â”‚
  â”‚   â”‚   Example: [8, 4, 768]
  â”‚   â”‚   â†’ [8, 4, 12, 64]
  â”‚   â”‚   â†’ [8, 12, 4, 64]
  â”‚   â”‚
  â”‚   â””â”€â–º Now we have 12 independent attention heads!
  â”‚
  â””â”€â–º Each head processes a 64-dimensional subspace


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Parallel Attention for Each Head                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚   FOR EACH HEAD (computed in parallel):
  â”‚
  â”œâ”€â–º Compute attention scores:
  â”‚   attn_scores = Q @ K.transpose(-2, -1)
  â”‚   Shape: [batch, num_heads, num_tokens, num_tokens]
  â”‚   Example: [8, 12, 4, 4]
  â”‚
  â”œâ”€â–º Apply causal mask:
  â”‚   attn_scores.masked_fill_(mask, -inf)
  â”‚
  â”œâ”€â–º Scale and softmax:
  â”‚   attn_weights = softmax(attn_scores / sqrt(head_dim), dim=-1)
  â”‚
  â””â”€â–º Compute context:
      context = attn_weights @ V
      Shape: [batch, num_heads, num_tokens, head_dim]
      Example: [8, 12, 4, 64]


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Concatenate Heads                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º Transpose back:
  â”‚   [batch, num_heads, num_tokens, head_dim]
  â”‚   â†’ [batch, num_tokens, num_heads, head_dim]
  â”‚   â”‚
  â”‚   Example: [8, 12, 4, 64] â†’ [8, 4, 12, 64]
  â”‚
  â”œâ”€â–º Flatten heads:
  â”‚   [batch, num_tokens, num_heads, head_dim]
  â”‚   â†’ [batch, num_tokens, num_heads * head_dim]
  â”‚   â†’ [batch, num_tokens, d_out]
  â”‚   â”‚
  â”‚   Example: [8, 4, 12, 64] â†’ [8, 4, 768]
  â”‚
  â””â”€â–º Concatenated output from all heads


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Output Projection                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â””â”€â–º out_proj = Linear(d_out, d_out)
      output = out_proj(concatenated)
      â”‚
      â””â”€â–º Final Shape: [batch, num_tokens, d_out]
          Example: [8, 4, 768]

OUTPUT: Context-aware representations with multi-head attention!
```

---

## Detailed Code Flow

### Single-Head Causal Self-Attention

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):
        super().__init__()
        self.d_out = d_out

        # Linear projections for Q, K, V
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)

        # Causal mask (upper triangular)
        self.register_buffer(
            'mask',
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        # x shape: [batch, num_tokens, d_in]
        b, num_tokens, d_in = x.shape

        # STEP 1: Project to Q, K, V
        keys    = self.W_key(x)    # [batch, num_tokens, d_out]
        queries = self.W_query(x)  # [batch, num_tokens, d_out]
        values  = self.W_value(x)  # [batch, num_tokens, d_out]

        # STEP 2: Compute attention scores
        # queries: [b, num_tokens, d_out]
        # keys.T:  [b, d_out, num_tokens]
        # result:  [b, num_tokens, num_tokens]
        attn_scores = queries @ keys.transpose(1, 2)

        # STEP 3: Apply causal mask
        # Prevent attending to future positions
        attn_scores.masked_fill_(
            self.mask.bool()[:num_tokens, :num_tokens],
            -torch.inf
        )

        # STEP 4: Scale and normalize
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5,
            dim=-1
        )
        attn_weights = self.dropout(attn_weights)

        # STEP 5: Compute context vectors
        context_vec = attn_weights @ values
        # [batch, num_tokens, num_tokens] @ [batch, num_tokens, d_out]
        # = [batch, num_tokens, d_out]

        return context_vec
```

### Multi-Head Attention (Efficient Implementation)

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads

        # Single linear layers for all heads combined
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Output projection
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)

        # Causal mask
        self.register_buffer(
            'mask',
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        # STEP 1: Project to Q, K, V
        keys    = self.W_key(x)    # [b, num_tokens, d_out]
        queries = self.W_query(x)  # [b, num_tokens, d_out]
        values  = self.W_value(x)  # [b, num_tokens, d_out]

        # STEP 2: Reshape for multi-head
        # Split d_out into (num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose to [b, num_heads, num_tokens, head_dim]
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # STEP 3: Compute attention (for all heads in parallel)
        attn_scores = queries @ keys.transpose(2, 3)
        # [b, num_heads, num_tokens, num_tokens]

        # STEP 4: Apply mask
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        # STEP 5: Softmax
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5,
            dim=-1
        )
        attn_weights = self.dropout(attn_weights)

        # STEP 6: Compute context vectors
        context_vec = attn_weights @ values
        # [b, num_heads, num_tokens, head_dim]

        # STEP 7: Concatenate heads
        context_vec = context_vec.transpose(1, 2)
        # [b, num_tokens, num_heads, head_dim]

        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        # [b, num_tokens, d_out]

        # STEP 8: Output projection
        context_vec = self.out_proj(context_vec)

        return context_vec
```

---

## Visual Attention Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HOW ATTENTION WORKS: CONCRETE EXAMPLE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input sentence: "The cat sat on the mat"
Tokens: ["The", "cat", "sat", "on", "the", "mat"]

For token "sat" (position 2):

STEP 1: Compute similarity with all previous tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Query("sat") Â· Key("The")  = 0.1
  Query("sat") Â· Key("cat")  = 0.8
  Query("sat") Â· Key("sat")  = 0.5
  Query("sat") Â· Key("on")   = -inf  (masked, future token)
  Query("sat") Â· Key("the")  = -inf  (masked, future token)
  Query("sat") Â· Key("mat")  = -inf  (masked, future token)

STEP 2: Apply softmax (convert to probabilities)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  After softmax:
    "The": 0.1  (10% attention)
    "cat": 0.7  (70% attention)  â† Most attention here!
    "sat": 0.2  (20% attention)

STEP 3: Weighted sum of values
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  context("sat") = 0.1 * Value("The")
                 + 0.7 * Value("cat")
                 + 0.2 * Value("sat")

Result: "sat" now has strong representation of "cat" context!

This makes sense: "cat" is the subject that's doing the sitting.
```

---

## Tensor Shape Transformations

| Stage | Shape | Description |
|-------|-------|-------------|
| Input Embeddings | `[8, 4, 256]` | Batch of 8, 4 tokens, 256-dim |
| After Q/K/V projection | `[8, 4, 256]` | Same shape, different subspace |
| Attention Scores | `[8, 4, 4]` | Token-to-token similarities |
| After Masking | `[8, 4, 4]` | Future positions = -inf |
| Attention Weights | `[8, 4, 4]` | Probability distributions |
| Context Vectors | `[8, 4, 256]` | Weighted combinations |
| **Multi-Head** | | |
| After reshape for heads | `[8, 12, 4, 64]` | 12 heads, 64-dim each |
| Attention per head | `[8, 12, 4, 4]` | Separate attention per head |
| Context per head | `[8, 12, 4, 64]` | Separate context per head |
| After concatenation | `[8, 4, 768]` | All heads combined |
| After output projection | `[8, 4, 768]` | Final multi-head output |

---

## Key Hyperparameters

```python
# Attention dimensions
d_in = 256          # Input dimension (embedding size)
d_out = 256         # Output dimension
num_heads = 12      # Number of attention heads
head_dim = 64       # d_out / num_heads

# Context settings
context_length = 1024   # Maximum sequence length
dropout = 0.1           # Dropout rate for regularization

# Optional settings
qkv_bias = False    # Whether to use bias in Q/K/V projections
```

---

## Why Multi-Head Attention?

**Single Head:**
- Learns one type of relationship
- Limited representational capacity

**Multi-Head:**
- Each head can specialize in different patterns:
  - Head 1: Subject-verb relationships
  - Head 2: Noun-adjective relationships
  - Head 3: Long-range dependencies
  - Head 4: Local context
  - etc.
- More expressive
- Better generalization

---

## Important Implementation Details

1. **Causal Masking**: Essential for autoregressive models (GPT). Prevents information leakage from future tokens.

2. **Scaling Factor**: `1/sqrt(d_out)` prevents softmax saturation with large embedding dimensions.

3. **Dropout**: Applied to attention weights for regularization, not to context vectors directly.

4. **Buffer vs Parameter**: Mask is registered as buffer (not updated during training), Q/K/V are parameters.

5. **Efficient Multi-Head**: Single projection matrix split into heads is more efficient than multiple separate projections.

---

## Complete Pipeline

```
Input Text
    â†“
[Ch02: Tokenization & Embedding]
    â†“
Token Embeddings [batch, tokens, d_in]
    â†“
[Ch03: Multi-Head Attention]
    â”œâ”€â–º Linear Projection to Q, K, V
    â”œâ”€â–º Reshape for Multiple Heads
    â”œâ”€â–º Compute Attention Scores
    â”œâ”€â–º Apply Causal Mask
    â”œâ”€â–º Scale and Softmax
    â”œâ”€â–º Weighted Sum of Values
    â”œâ”€â–º Concatenate Heads
    â””â”€â–º Output Projection
    â†“
Context Vectors [batch, tokens, d_out]
    â†“
Ready for Transformer Block (Ch04)! ğŸš€
```

---

## Code Location

- **Main notebook**: `ch03/01_main-chapter-code/ch03.ipynb`
- **Condensed version**: `ch03/01_main-chapter-code/multihead-attention.ipynb`
- **Exercises**: `ch03/01_main-chapter-code/exercise-solutions.ipynb`

---

## Next Steps

After completing Chapter 3, you'll have:
- âœ… Implemented self-attention mechanism
- âœ… Added causal masking for autoregressive modeling
- âœ… Built multi-head attention for richer representations
- âœ… Understood scaled dot-product attention

**Ready for Chapter 4**: Building the complete GPT model! ğŸš€
