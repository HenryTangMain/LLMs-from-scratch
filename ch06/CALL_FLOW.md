# Chapter 6: Finetuning for Text Classification - Call Flow Diagram

This document provides a detailed call flow diagram for finetuning a GPT model for text classification in Chapter 6.

## Overview

Chapter 6 demonstrates how to adapt a pretrained language model for a specific classification task (spam detection). It covers:

1. **Classification Head** - Adding task-specific output layer
2. **Dataset Preparation** - Loading and preprocessing labeled data
3. **Freezing Layers** - Selective parameter training
4. **Classification Training** - Modified training loop for classification
5. **Accuracy Evaluation** - Computing classification metrics
6. **Full Model Finetuning** - Training all parameters

---

## Classification vs Language Modeling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LANGUAGE MODEL vs CLASSIFIER                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LANGUAGE MODEL (Ch05):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Input:  "The cat sat"
          [token_0, token_1, token_2]
          â†“
  Model:  GPT (all layers)
          â†“
  Output: Logits for EACH position
          [vocab_size scores] for token_0 â†’ predicts token_1
          [vocab_size scores] for token_1 â†’ predicts token_2
          [vocab_size scores] for token_2 â†’ predicts token_3
          â†“
  Loss:   Cross-entropy for next-token prediction
          Average across all positions

  Purpose: Generate coherent text


CLASSIFIER (Ch06):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Input:  "Buy now! Limited offer!"
          [token_0, token_1, ..., token_n]
          â†“
  Model:  GPT (all layers) + Classification head
          â†“
  Output: Logits for LAST position only
          [num_classes scores]  â† e.g., [spam_score, ham_score]
          â†“
  Loss:   Cross-entropy for classification
          Single prediction per input

  Purpose: Classify entire text into categories
```

---

## Complete Classification Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CH06: FINETUNING FOR CLASSIFICATION                       â”‚
â”‚                   Classification Pipeline Flow                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: DATA PREPARATION                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º Download spam dataset
  â”‚   URL: SMS Spam Collection
  â”‚   Format: "ham/spam \t message text"
  â”‚
  â”œâ”€â–º Load into DataFrame
  â”‚   df = pd.read_csv("sms_spam_collection.tsv", sep="\t")
  â”‚   â”‚
  â”‚   â”‚   Label    Text
  â”‚   â”‚   â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚   â”‚   ham      "How are you doing today?"
  â”‚   â”‚   spam     "WINNER! Claim your prize now!"
  â”‚   â”‚   ham      "See you at the meeting"
  â”‚   â”‚   spam     "Call now for free offer!!!"
  â”‚   â”‚   ...
  â”‚
  â”œâ”€â–º Balance dataset (equal spam/ham)
  â”‚   num_spam = df[df["Label"] == "spam"].shape[0]
  â”‚   ham_subset = df[df["Label"] == "ham"].sample(num_spam)
  â”‚   balanced_df = pd.concat([ham_subset, spam_df])
  â”‚   â”‚
  â”‚   â””â”€â–º Prevents class imbalance bias
  â”‚
  â”œâ”€â–º Encode labels
  â”‚   balanced_df["Label"] = balanced_df["Label"].map(
  â”‚       {"ham": 0, "spam": 1}
  â”‚   )
  â”‚
  â””â”€â–º Split into train/val/test
      train_df, val_df, test_df = random_split(
          balanced_df,
          train_frac=0.7,
          validation_frac=0.1
      )
      # test_frac = 0.2 (remaining)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: DATASET CLASS FOR CLASSIFICATION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):
        â”‚
        â”œâ”€â–º Load data
        â”‚   self.data = pd.read_csv(csv_file)
        â”‚
        â”œâ”€â–º Tokenize all texts
        â”‚   self.encoded_texts = [
        â”‚       tokenizer.encode(text) for text in self.data["Text"]
        â”‚   ]
        â”‚
        â”œâ”€â–º Determine max_length
        â”‚   if max_length is None:
        â”‚       self.max_length = max(len(text) for text in self.encoded_texts)
        â”‚   else:
        â”‚       self.max_length = max_length
        â”‚       # Truncate sequences longer than max_length
        â”‚       self.encoded_texts = [
        â”‚           text[:max_length] for text in self.encoded_texts
        â”‚       ]
        â”‚
        â””â”€â–º Pad all sequences to max_length
            self.encoded_texts = [
                text + [pad_token_id] * (max_length - len(text))
                for text in self.encoded_texts
            ]

    def __getitem__(self, index):
        """Return (encoded_text, label) pair"""
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)


Example:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Text: "WINNER! Call now!"
Tokens: [WIN, NER, !, Call, now, !]
Token IDs: [12345, 678, 0, 2345, 890, 0]

After padding (max_length=10):
  [12345, 678, 0, 2345, 890, 0, 50256, 50256, 50256, 50256]
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ actual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€ padding â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Label: 1 (spam)
```

---

## Model Architecture Modification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLASSIFICATION HEAD                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ORIGINAL GPT MODEL (Language Modeling):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Input tokens â†’ Embeddings â†’ Transformer Blocks â†’ Final LayerNorm
                                                          â†“
                                                    Linear(emb_dim, vocab_size)
                                                          â†“
                                                    Logits [batch, seq_len, 50257]


MODIFIED FOR CLASSIFICATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Input tokens â†’ Embeddings â†’ Transformer Blocks â†’ Final LayerNorm
                                                          â†“
                            Select LAST token representation â†â”€â”
                                    x[:, -1, :]                â”‚
                                          â†“                    â”‚
                            Linear(emb_dim, num_classes) â†â”€â”€â”€â”â”‚
                                          â†“                   â”‚â”‚
                            Logits [batch, num_classes]      â”‚â”‚
                                                              â”‚â”‚
Why last token?                                               â”‚â”‚
  - Causal attention means last token "sees" all previous    â”‚â”‚
  - Last token has full context of entire sequence           â”‚â”‚
  - Common practice in GPT-based classification              â”‚â”‚
                                                              â”‚â”‚
  Input:  [CLS] [token1] [token2] ... [tokenN]               â”‚â”‚
           â†“       â†“        â†“           â†“                     â”‚â”‚
          attended â†’ attended â†’ attended â†’ [LAST] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                                            â†“                  â”‚
                                  Use this for classification â”€â”˜


MODEL FORWARD PASS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def forward(self, in_idx):
    # Standard GPT forward pass
    batch_size, seq_len = in_idx.shape
    x = self.tok_emb(in_idx) + self.pos_emb(torch.arange(seq_len))
    x = self.drop_emb(x)
    x = self.trf_blocks(x)
    x = self.final_norm(x)

    # Classification: use LAST token only
    logits = self.out_head(x[:, -1, :])  # [batch, seq_len, emb_dim]
                                          #         â†“
                                          # [batch, emb_dim]
                                          #         â†“
                                          # [batch, num_classes]
    return logits
```

---

## Training Strategy: Freezing Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER FREEZING STRATEGY                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STRATEGY 1: Freeze All Transformer Blocks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Freeze: All transformer blocks (pretrained knowledge)
  Train:  Only classification head (task-specific)

  # Freeze transformer blocks
  for param in model.trf_blocks.parameters():
      param.requires_grad = False

  # Classification head remains trainable
  for param in model.out_head.parameters():
      param.requires_grad = True

  Advantages:
    âœ“ Fast training (fewer parameters)
    âœ“ Less risk of overfitting
    âœ“ Works well with small datasets

  Disadvantages:
    âœ— Limited adaptation to new domain
    âœ— May underperform on very different tasks


STRATEGY 2: Freeze Lower Layers Only
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Freeze: First 6 transformer blocks (general features)
  Train:  Last 6 blocks + classification head (task adaptation)

  # Freeze first 6 blocks
  for block in model.trf_blocks[:6]:
      for param in block.parameters():
          param.requires_grad = False

  # Unfreeze last 6 blocks
  for block in model.trf_blocks[6:]:
      for param in block.parameters():
          param.requires_grad = True

  Advantages:
    âœ“ Better task adaptation
    âœ“ Still relatively fast
    âœ“ Good balance

  Disadvantages:
    âœ— More parameters to tune
    âœ— Slightly higher risk of overfitting


STRATEGY 3: Train All Layers (Full Finetuning)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Freeze: Nothing
  Train:  All parameters

  # Unfreeze everything
  for param in model.parameters():
      param.requires_grad = True

  Advantages:
    âœ“ Maximum task adaptation
    âœ“ Best potential performance
    âœ“ Can adapt to very different domains

  Disadvantages:
    âœ— Slow training (many parameters)
    âœ— High risk of overfitting with small data
    âœ— Requires careful hyperparameter tuning


PARAMETER COUNT COMPARISON (GPT-2 124M):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Strategy 1 (classification head only):     ~1.5K parameters
  Strategy 2 (last 6 blocks + head):        ~44M parameters
  Strategy 3 (all layers):                  ~124M parameters
```

---

## Classification Training Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLASSIFICATION TRAINING                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

def train_classifier_simple(model, train_loader, val_loader, optimizer,
                            device, num_epochs, eval_freq, eval_iter):
    â”‚
    â”œâ”€â–º Initialize tracking
    â”‚   train_losses, val_losses = [], []
    â”‚   train_accs, val_accs = [], []
    â”‚   examples_seen, global_step = 0, -1
    â”‚
    â””â”€â–º Main training loop
        for epoch in range(num_epochs):
            model.train()

            for input_batch, target_batch in train_loader:
                â”‚
                â”œâ”€â–º Forward pass
                â”‚   input_batch = input_batch.to(device)
                â”‚   target_batch = target_batch.to(device)
                â”‚   â”‚
                â”‚   â”‚   input_batch: [batch_size, seq_len]
                â”‚   â”‚   target_batch: [batch_size]  â† Single label per input
                â”‚   â”‚
                â”‚   logits = model(input_batch)
                â”‚   â”‚
                â”‚   â””â”€â–º logits: [batch_size, num_classes]
                â”‚       Example: [8, 2]  (2 classes: ham, spam)
                â”‚
                â”œâ”€â–º Compute loss
                â”‚   loss = calc_loss_batch(
                â”‚       input_batch, target_batch, model, device
                â”‚   )
                â”‚   â”‚
                â”‚   â”‚   Inside calc_loss_batch:
                â”‚   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                â”‚   â”‚   logits = model(input_batch)[:, -1, :]
                â”‚   â”‚   loss = F.cross_entropy(logits, target_batch)
                â”‚   â”‚
                â”‚   â””â”€â–º Single classification loss (not per-token)
                â”‚
                â”œâ”€â–º Backward pass
                â”‚   optimizer.zero_grad()
                â”‚   loss.backward()
                â”‚   optimizer.step()
                â”‚
                â”œâ”€â–º Update counters
                â”‚   examples_seen += input_batch.shape[0]
                â”‚   global_step += 1
                â”‚
                â””â”€â–º Periodic evaluation
                    if global_step % eval_freq == 0:
                        train_loss, val_loss = evaluate_model(...)
                        train_acc = calc_accuracy_loader(train_loader, ...)
                        val_acc = calc_accuracy_loader(val_loader, ...)

                        train_losses.append(train_loss)
                        val_losses.append(val_loss)
                        train_accs.append(train_acc)
                        val_accs.append(val_acc)

                        print(f"Ep {epoch+1} (Step {global_step:06d}): "
                              f"Train loss {train_loss:.3f}, "
                              f"Val loss {val_loss:.3f}, "
                              f"Train acc {train_acc:.2f}, "
                              f"Val acc {val_acc:.2f}")

        return train_losses, val_losses, train_accs, val_accs, examples_seen
```

---

## Accuracy Calculation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPUTING CLASSIFICATION ACCURACY                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    """
    Compute classification accuracy.

    Returns: Fraction of correct predictions
    """
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i >= num_batches:
            break

        input_batch = input_batch.to(device)
        target_batch = target_batch.to(device)

        with torch.no_grad():
            # Get logits for last token
            logits = model(input_batch)[:, -1, :]
            # Shape: [batch_size, num_classes]

            # Get predicted class (highest score)
            predicted_labels = torch.argmax(logits, dim=-1)
            # Shape: [batch_size]

            # Compare with true labels
            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()

    return correct_predictions / num_examples


Example:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Batch size: 4

Logits: [[ 2.3, -1.2],  â† Text 1: ham score=2.3, spam score=-1.2
         [-0.5,  3.1],  â† Text 2: ham score=-0.5, spam score=3.1
         [ 1.8, -0.9],  â† Text 3: ham score=1.8, spam score=-0.9
         [-1.0,  2.7]]  â† Text 4: ham score=-1.0, spam score=2.7

Predicted: [0, 1, 0, 1]  â† argmax of each row
           (ham, spam, ham, spam)

Target:    [0, 1, 1, 1]  â† True labels
           (ham, spam, spam, spam)

Correct:   [âœ“, âœ“, âœ—, âœ“]  â† 3 out of 4 correct

Accuracy: 3/4 = 0.75 = 75%
```

---

## Complete Classification Example

```python
# ============================================
# STEP 1: Load Pretrained Model
# ============================================
from gpt_download import download_and_load_gpt2
from previous_chapters import GPTModel, load_weights_into_gpt

# Base configuration
BASE_CONFIG = {
    "vocab_size": 50257,
    "context_length": 1024,
    "drop_rate": 0.0,
    "qkv_bias": True
}

# Model sizes
model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

# Choose model
CHOOSE_MODEL = "gpt2-medium (355M)"
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

# Load weights
settings, params = download_and_load_gpt2(
    model_size="355M",
    models_dir="gpt2"
)

# Initialize model
model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()


# ============================================
# STEP 2: Modify for Classification
# ============================================
# Replace output head for classification
num_classes = 2
model.out_head = torch.nn.Linear(
    in_features=BASE_CONFIG["emb_dim"],
    out_features=num_classes
)

# Move to device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)


# ============================================
# STEP 3: Freeze Layers (Optional)
# ============================================
# Option 1: Freeze all transformer blocks
for param in model.trf_blocks.parameters():
    param.requires_grad = False

# Option 2: Freeze only first 6 blocks
# for block in model.trf_blocks[:6]:
#     for param in block.parameters():
#         param.requires_grad = False

# Classification head is always trainable
for param in model.out_head.parameters():
    param.requires_grad = True


# ============================================
# STEP 4: Prepare Data
# ============================================
import tiktoken

tokenizer = tiktoken.get_encoding("gpt2")

# Download and prepare spam dataset
download_and_unzip_spam_data(
    url="https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip",
    zip_path="sms_spam_collection.zip",
    extracted_path="sms_spam_collection",
    data_file_path="sms_spam_collection.tsv"
)

# Load and balance
df = pd.read_csv("sms_spam_collection.tsv", sep="\t", header=None, names=["Label", "Text"])
balanced_df = create_balanced_dataset(df)
balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})

# Split
train_df, val_df, test_df = random_split(balanced_df, 0.7, 0.1)
train_df.to_csv("train.csv", index=None)
val_df.to_csv("val.csv", index=None)
test_df.to_csv("test.csv", index=None)

# Create datasets
train_dataset = SpamDataset("train.csv", tokenizer, max_length=120)
val_dataset = SpamDataset("val.csv", tokenizer, max_length=120)
test_dataset = SpamDataset("test.csv", tokenizer, max_length=120)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)


# ============================================
# STEP 5: Train
# ============================================
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)

num_epochs = 5
train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    device=device,
    num_epochs=num_epochs,
    eval_freq=50,
    eval_iter=5
)


# ============================================
# STEP 6: Evaluate
# ============================================
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy:   {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy:       {test_accuracy*100:.2f}%")


# ============================================
# STEP 7: Inference on New Text
# ============================================
def classify_review(text, model, tokenizer, device, max_length=None):
    model.eval()

    # Tokenize and pad
    input_ids = tokenizer.encode(text)
    if max_length is not None:
        input_ids = input_ids[:max_length]

    # Create batch dimension
    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)

    # Predict
    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]  # Last token

    # Get predicted class
    predicted_label = torch.argmax(logits, dim=-1).item()

    return "spam" if predicted_label == 1 else "ham"

# Test on new examples
text_1 = "You are a winner! Call now to claim your prize!"
text_2 = "Hey, can we meet for lunch tomorrow?"

print(f"Text 1: {classify_review(text_1, model, tokenizer, device, max_length=120)}")
print(f"Text 2: {classify_review(text_2, model, tokenizer, device, max_length=120)}")
```

---

## Training Monitoring

```
Typical Output:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ep 1 (Step 000050): Train loss 0.433, Val loss 0.287, Train acc 0.78, Val acc 0.85
Ep 1 (Step 000100): Train loss 0.312, Val loss 0.235, Train acc 0.86, Val acc 0.89
Ep 2 (Step 000150): Train loss 0.256, Val loss 0.201, Train acc 0.90, Val acc 0.92
Ep 2 (Step 000200): Train loss 0.198, Val loss 0.178, Train acc 0.93, Val acc 0.94
Ep 3 (Step 000250): Train loss 0.167, Val loss 0.165, Train acc 0.95, Val acc 0.95
...

Final Results:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training accuracy:   97.20%
Validation accuracy: 95.50%
Test accuracy:       96.10%

Good signs:
  âœ“ Validation accuracy close to training accuracy
  âœ“ Test accuracy similar to validation accuracy
  âœ“ High accuracy (>95%) on both classes

Warning signs:
  âœ— Train accuracy >> Val accuracy â†’ Overfitting
  âœ— Very low accuracy â†’ Model not learning or data issues
  âœ— One class accuracy much higher â†’ Class imbalance
```

---

## Key Differences from Language Modeling

| Aspect | Language Modeling (Ch05) | Classification (Ch06) |
|--------|-------------------------|----------------------|
| **Task** | Predict next token | Classify entire sequence |
| **Output** | All token positions | Last token only |
| **Logits shape** | `[B, T, V]` | `[B, C]` |
| **Loss** | Average over all positions | Single loss per input |
| **Labels** | Next tokens (self-supervised) | External labels (supervised) |
| **Evaluation** | Perplexity, generation quality | Accuracy, F1, precision/recall |
| **Training data** | Unlabeled text | Labeled examples |
| **Dataset size** | Large (millions of tokens) | Small (thousands of examples) |

Legend: B=batch, T=sequence_length, V=vocab_size, C=num_classes

---

## Code Location

- **Main notebook**: `ch06/01_main-chapter-code/ch06.ipynb`
- **Classification script**: `ch06/01_main-chapter-code/gpt_class_finetune.py`
- **Weight download**: `ch06/01_main-chapter-code/gpt_download.py`
- **Previous chapters**: `ch06/01_main-chapter-code/previous_chapters.py`

---

## Next Steps

After completing Chapter 6, you'll have:
- âœ… Finetuned a GPT model for classification
- âœ… Learned layer freezing strategies
- âœ… Implemented classification-specific data loading
- âœ… Computed accuracy metrics
- âœ… Applied pretrained models to new tasks

**Ready for Chapter 7**: Instruction finetuning! ðŸš€
