# Chapter 5: Pretraining on Unlabeled Data - Call Flow Diagram

This document provides a detailed call flow diagram for the training pipeline in Chapter 5.

## Overview

Chapter 5 implements the complete training loop for pretraining a GPT model on unlabeled text data. It covers:

1. **Loss Calculation** - Computing cross-entropy loss for next-token prediction
2. **Training Loop** - Iterative optimization process
3. **Evaluation** - Monitoring training and validation performance
4. **Weight Loading** - Loading pretrained GPT-2 weights
5. **Advanced Generation** - Temperature sampling and top-k sampling
6. **Model Saving/Loading** - Checkpointing trained models

---

## Complete Training Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CH05: PRETRAINING ON UNLABELED DATA                    â”‚
â”‚                        Training Pipeline Flow                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: SETUP                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º Load and prepare data
  â”‚   with open("the-verdict.txt", "r") as f:
  â”‚       text_data = f.read()
  â”‚
  â”œâ”€â–º Split into train/validation
  â”‚   train_ratio = 0.90
  â”‚   split_idx = int(train_ratio * len(text_data))
  â”‚   train_data = text_data[:split_idx]
  â”‚   val_data = text_data[split_idx:]
  â”‚
  â”œâ”€â–º Create dataloaders (from Ch02)
  â”‚   train_loader = create_dataloader_v1(
  â”‚       train_data, batch_size=2, max_length=256, stride=256
  â”‚   )
  â”‚   val_loader = create_dataloader_v1(
  â”‚       val_data, batch_size=2, max_length=256, stride=256
  â”‚   )
  â”‚
  â”œâ”€â–º Initialize model (from Ch04)
  â”‚   model = GPTModel(GPT_CONFIG_124M)
  â”‚   model.to(device)
  â”‚
  â””â”€â–º Initialize optimizer
      optimizer = torch.optim.AdamW(
          model.parameters(),
          lr=0.0004,
          weight_decay=0.1
      )


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: TRAINING LOOP (Main)                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  FOR epoch IN range(num_epochs):
    â”‚
    â”œâ”€â–º Set model to training mode
    â”‚   model.train()
    â”‚
    â”‚   FOR batch IN train_loader:
    â”‚     â”‚
    â”‚     â”œâ”€â–º Get input and target from batch
    â”‚     â”‚   input_batch, target_batch = batch
    â”‚     â”‚   â”‚
    â”‚     â”‚   â”‚   input_batch:  [batch_size, seq_len]
    â”‚     â”‚   â”‚   target_batch: [batch_size, seq_len]
    â”‚     â”‚   â”‚   (targets = inputs shifted by 1)
    â”‚     â”‚
    â”‚     â”œâ”€â–º FORWARD PASS
    â”‚     â”‚   â”‚
    â”‚     â”‚   â”œâ”€â–º Zero gradients from previous step
    â”‚     â”‚   â”‚   optimizer.zero_grad()
    â”‚     â”‚   â”‚
    â”‚     â”‚   â”œâ”€â–º Forward through model
    â”‚     â”‚   â”‚   logits = model(input_batch)
    â”‚     â”‚   â”‚   â”‚
    â”‚     â”‚   â”‚   â””â”€â–º Shape: [batch_size, seq_len, vocab_size]
    â”‚     â”‚   â”‚
    â”‚     â”‚   â””â”€â–º Compute loss
    â”‚     â”‚       loss = calc_loss_batch(input_batch, target_batch, model)
    â”‚     â”‚
    â”‚     â”œâ”€â–º BACKWARD PASS
    â”‚     â”‚   â”‚
    â”‚     â”‚   â”œâ”€â–º Compute gradients
    â”‚     â”‚   â”‚   loss.backward()
    â”‚     â”‚   â”‚
    â”‚     â”‚   â””â”€â–º Update weights
    â”‚     â”‚       optimizer.step()
    â”‚     â”‚
    â”‚     â””â”€â–º Optional: Evaluate every N steps
    â”‚         if global_step % eval_freq == 0:
    â”‚             evaluate_model(model, train_loader, val_loader)
    â”‚
    â””â”€â–º Optional: Generate sample text after epoch
        generate_and_print_sample(model, tokenizer, device, "Every effort moves you")


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: LOSS CALCULATION (Next-Token Prediction)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

def calc_loss_batch(input_batch, target_batch, model, device):
    â”‚
    â”œâ”€â–º Move data to device
    â”‚   input_batch = input_batch.to(device)
    â”‚   target_batch = target_batch.to(device)
    â”‚
    â”œâ”€â–º Forward pass
    â”‚   logits = model(input_batch)
    â”‚   â”‚
    â”‚   â””â”€â–º Shape: [batch_size, seq_len, vocab_size]
    â”‚       Example: [8, 256, 50257]
    â”‚
    â”œâ”€â–º Flatten for loss computation
    â”‚   logits_flat = logits.flatten(0, 1)
    â”‚   targets_flat = target_batch.flatten()
    â”‚   â”‚
    â”‚   â”‚   Before: logits  [8, 256, 50257]
    â”‚   â”‚           targets [8, 256]
    â”‚   â”‚
    â”‚   â”‚   After:  logits  [2048, 50257]  â† 8*256 = 2048
    â”‚   â”‚           targets [2048]
    â”‚   â”‚
    â”‚   â””â”€â–º Treat all positions as independent predictions
    â”‚
    â””â”€â–º Compute cross-entropy loss
        loss = F.cross_entropy(logits_flat, targets_flat)
        â”‚
        â””â”€â–º Measures how well model predicts next token

    return loss


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CROSS-ENTROPY LOSS DETAILS                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For each token position:
  â”‚
  â”œâ”€â–º Model outputs logits (unnormalized scores)
  â”‚   logits = [sâ‚€, sâ‚, sâ‚‚, ..., sâ‚…â‚€â‚‚â‚…â‚†]  â† score for each vocab token
  â”‚
  â”œâ”€â–º Convert to probabilities
  â”‚   probs = softmax(logits)
  â”‚   probs = [pâ‚€, pâ‚, pâ‚‚, ..., pâ‚…â‚€â‚‚â‚…â‚†]  â† sums to 1
  â”‚
  â”œâ”€â–º Compare to actual next token
  â”‚   target = 3797  (e.g., token ID for "cat")
  â”‚
  â””â”€â–º Compute loss
      loss = -log(probs[target])
      â”‚
      â”‚   If probs[3797] = 0.8  â†’ loss = -log(0.8) = 0.22  â† good
      â”‚   If probs[3797] = 0.1  â†’ loss = -log(0.1) = 2.30  â† bad
      â”‚   If probs[3797] = 0.01 â†’ loss = -log(0.01) = 4.61 â† very bad
      â”‚
      â””â”€â–º Lower loss = better prediction
```

---

## Evaluation Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL EVALUATION                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    â”‚
    â”œâ”€â–º Set model to evaluation mode
    â”‚   model.eval()
    â”‚   â”‚
    â”‚   â””â”€â–º Disables dropout, puts BatchNorm in eval mode
    â”‚
    â”œâ”€â–º Disable gradient computation (saves memory)
    â”‚   with torch.no_grad():
    â”‚       â”‚
    â”‚       â”œâ”€â–º Evaluate on training data
    â”‚       â”‚   train_loss = calc_loss_loader(
    â”‚       â”‚       train_loader, model, device, num_batches=eval_iter
    â”‚       â”‚   )
    â”‚       â”‚
    â”‚       â””â”€â–º Evaluate on validation data
    â”‚           val_loss = calc_loss_loader(
    â”‚               val_loader, model, device, num_batches=eval_iter
    â”‚           )
    â”‚
    â”œâ”€â–º Set model back to training mode
    â”‚   model.train()
    â”‚
    â””â”€â–º Return losses
        return train_loss, val_loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    """Compute average loss over entire dataloader"""
    â”‚
    â”œâ”€â–º Initialize accumulator
    â”‚   total_loss = 0.
    â”‚
    â”œâ”€â–º Iterate through batches
    â”‚   for i, (input_batch, target_batch) in enumerate(data_loader):
    â”‚       â”‚
    â”‚       â”œâ”€â–º Stop after num_batches (if specified)
    â”‚       â”‚   if i >= num_batches:
    â”‚       â”‚       break
    â”‚       â”‚
    â”‚       â”œâ”€â–º Compute loss for this batch
    â”‚       â”‚   loss = calc_loss_batch(input_batch, target_batch, model, device)
    â”‚       â”‚
    â”‚       â””â”€â–º Accumulate
    â”‚           total_loss += loss.item()
    â”‚
    â””â”€â–º Return average
        return total_loss / num_batches
```

---

## Advanced Text Generation

### Temperature Sampling

```python
def generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):
    """
    Generate text with temperature and top-k sampling.

    Temperature controls randomness:
      - temperature < 1: More conservative (peaked distribution)
      - temperature = 1: Use raw probabilities
      - temperature > 1: More random (flatter distribution)

    Top-k limits sampling to k most likely tokens
    """
    for _ in range(max_new_tokens):
        # Crop context to model's max length
        idx_cond = idx[:, -context_size:]

        # Get logits
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]  # Focus on last position

        # Apply top-k filtering
        if top_k is not None:
            # Keep only top k logits, set others to -inf
            top_logits, top_indices = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]  # k-th highest value
            logits = torch.where(
                logits < min_val,
                torch.tensor(float('-inf')).to(logits.device),
                logits
            )

        # Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Convert to probabilities and sample
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        else:
            # Greedy decoding (temperature = 0)
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)

        # Append to sequence
        idx = torch.cat([idx, idx_next], dim=1)

    return idx
```

**Temperature Effect:**
```
Original logits: [2.0, 1.0, 0.5, 0.1]

Temperature = 0.5 (more peaked):
  Scaled: [4.0, 2.0, 1.0, 0.2]
  Probs:  [0.73, 0.20, 0.06, 0.01]  â† More confident

Temperature = 1.0 (unchanged):
  Scaled: [2.0, 1.0, 0.5, 0.1]
  Probs:  [0.52, 0.19, 0.11, 0.07]  â† Balanced

Temperature = 2.0 (flatter):
  Scaled: [1.0, 0.5, 0.25, 0.05]
  Probs:  [0.38, 0.23, 0.18, 0.15]  â† More random
```

**Top-k Sampling:**
```
All logits: [2.0, 1.5, 1.0, 0.8, 0.5, 0.3, ...]
             â†‘    â†‘    â†‘    â†‘
             Keep top-4, set rest to -inf

After top-k (k=4):
  Filtered: [2.0, 1.5, 1.0, 0.8, -inf, -inf, ...]
  Probs:    [0.42, 0.28, 0.17, 0.13, 0.0, 0.0, ...]

Sample only from top 4 tokens
```

---

## Loading Pretrained Weights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOADING PRETRAINED GPT-2 WEIGHTS                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

def load_weights_into_gpt(gpt, params):
    """
    Load weights from pretrained GPT-2 checkpoint.

    params: Dictionary with structure:
      {
        "wte": token_embedding_weights,
        "wpe": position_embedding_weights,
        "blocks": [
          {
            "ln_1": {"g": scale, "b": shift},
            "attn": {...},
            "ln_2": {...},
            "mlp": {...}
          },
          ...
        ],
        "ln_f": final_layer_norm_weights
      }
    """
    â”‚
    â”œâ”€â–º Load embeddings
    â”‚   gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params["wte"])
    â”‚   gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params["wpe"])
    â”‚
    â”œâ”€â–º Load each transformer block
    â”‚   for i, block in enumerate(gpt.trf_blocks):
    â”‚       â”‚
    â”‚       â”œâ”€â–º Layer norm 1
    â”‚       â”‚   block.norm1.scale = assign(
    â”‚       â”‚       block.norm1.scale,
    â”‚       â”‚       params["blocks"][i]["ln_1"]["g"]
    â”‚       â”‚   )
    â”‚       â”‚   block.norm1.shift = assign(
    â”‚       â”‚       block.norm1.shift,
    â”‚       â”‚       params["blocks"][i]["ln_1"]["b"]
    â”‚       â”‚   )
    â”‚       â”‚
    â”‚       â”œâ”€â–º Attention weights
    â”‚       â”‚   q, k, v weights combined in GPT-2
    â”‚       â”‚   Need to split them:
    â”‚       â”‚   qkv_weight = params["blocks"][i]["attn"]["c_attn"]["w"]
    â”‚       â”‚   q_w, k_w, v_w = split(qkv_weight, 3)
    â”‚       â”‚   block.att.W_query.weight = assign(q_w)
    â”‚       â”‚   block.att.W_key.weight = assign(k_w)
    â”‚       â”‚   block.att.W_value.weight = assign(v_w)
    â”‚       â”‚
    â”‚       â”œâ”€â–º Output projection
    â”‚       â”‚   block.att.out_proj.weight = assign(
    â”‚       â”‚       params["blocks"][i]["attn"]["c_proj"]["w"]
    â”‚       â”‚   )
    â”‚       â”‚
    â”‚       â”œâ”€â–º Layer norm 2
    â”‚       â”‚   (Similar to layer norm 1)
    â”‚       â”‚
    â”‚       â””â”€â–º Feed-forward network
    â”‚           block.ff.layers[0].weight = assign(
    â”‚               params["blocks"][i]["mlp"]["c_fc"]["w"]
    â”‚           )
    â”‚           block.ff.layers[2].weight = assign(
    â”‚               params["blocks"][i]["mlp"]["c_proj"]["w"]
    â”‚           )
    â”‚
    â””â”€â–º Load final layer norm
        gpt.final_norm.scale = assign(params["ln_f"]["g"])
        gpt.final_norm.shift = assign(params["ln_f"]["b"])
```

---

## Training Progress Monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACKING TRAINING PROGRESS                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

def train_model_simple(model, train_loader, val_loader, optimizer,
                       device, num_epochs, eval_freq, eval_iter,
                       start_context, tokenizer):
    â”‚
    â”œâ”€â–º Initialize tracking lists
    â”‚   train_losses = []
    â”‚   val_losses = []
    â”‚   track_tokens_seen = []
    â”‚   tokens_seen = 0
    â”‚   global_step = -1
    â”‚
    â”œâ”€â–º Main training loop
    â”‚   for epoch in range(num_epochs):
    â”‚       model.train()
    â”‚
    â”‚       for input_batch, target_batch in train_loader:
    â”‚           â”‚
    â”‚           â”œâ”€â–º Training step
    â”‚           â”‚   optimizer.zero_grad()
    â”‚           â”‚   loss = calc_loss_batch(input_batch, target_batch, model, device)
    â”‚           â”‚   loss.backward()
    â”‚           â”‚   optimizer.step()
    â”‚           â”‚
    â”‚           â”œâ”€â–º Update counters
    â”‚           â”‚   tokens_seen += input_batch.numel()
    â”‚           â”‚   global_step += 1
    â”‚           â”‚
    â”‚           â””â”€â–º Periodic evaluation
    â”‚               if global_step % eval_freq == 0:
    â”‚                   train_loss, val_loss = evaluate_model(
    â”‚                       model, train_loader, val_loader, device, eval_iter
    â”‚                   )
    â”‚                   train_losses.append(train_loss)
    â”‚                   val_losses.append(val_loss)
    â”‚                   track_tokens_seen.append(tokens_seen)
    â”‚                   print(f"Ep {epoch+1} (Step {global_step:06d}): "
    â”‚                         f"Train loss {train_loss:.3f}, "
    â”‚                         f"Val loss {val_loss:.3f}")
    â”‚
    â”‚       # Generate sample after each epoch
    â”‚       generate_and_print_sample(model, tokenizer, device, start_context)
    â”‚
    â””â”€â–º Return training history
        return train_losses, val_losses, track_tokens_seen


Example output:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ep 1 (Step 000005): Train loss 9.781, Val loss 9.933
Ep 1 (Step 000010): Train loss 8.111, Val loss 8.339
Every effort moves you toward the goal of the project.

Ep 2 (Step 000015): Train loss 6.661, Val loss 7.048
Ep 2 (Step 000020): Train loss 5.802, Val loss 6.589
Every effort moves you closer to your final destination.

Ep 3 (Step 000025): Train loss 5.333, Val loss 6.200
...
```

---

## Model Checkpointing

```python
# Save model checkpoint
torch.save({
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    "epoch": epoch,
    "train_losses": train_losses,
    "val_losses": val_losses,
    "tokens_seen": tokens_seen
}, "model_checkpoint.pt")

# Load model checkpoint
checkpoint = torch.load("model_checkpoint.pt")
model.load_state_dict(checkpoint["model_state_dict"])
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
epoch = checkpoint["epoch"]
train_losses = checkpoint["train_losses"]
val_losses = checkpoint["val_losses"]
tokens_seen = checkpoint["tokens_seen"]
```

---

## Complete Training Example

```python
# Configuration
GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 256,      # Reduced for training
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}

TRAINING_CONFIG = {
    "learning_rate": 5e-4,
    "weight_decay": 0.1,
    "batch_size": 2,
    "num_epochs": 10
}

# Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GPTModel(GPT_CONFIG_124M)
model.to(device)

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=TRAINING_CONFIG["learning_rate"],
    weight_decay=TRAINING_CONFIG["weight_decay"]
)

# Load data
with open("the-verdict.txt", "r") as f:
    text_data = f.read()

train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))

train_loader = create_dataloader_v1(
    text_data[:split_idx],
    batch_size=TRAINING_CONFIG["batch_size"],
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True
)

val_loader = create_dataloader_v1(
    text_data[split_idx:],
    batch_size=TRAINING_CONFIG["batch_size"],
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False
)

# Train
tokenizer = tiktoken.get_encoding("gpt2")

train_losses, val_losses, tokens_seen = train_model_simple(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    device=device,
    num_epochs=TRAINING_CONFIG["num_epochs"],
    eval_freq=5,
    eval_iter=1,
    start_context="Every effort moves you",
    tokenizer=tokenizer
)

# Plot losses
import matplotlib.pyplot as plt
epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
plt.savefig("training_losses.pdf")

# Save model
torch.save(model.state_dict(), "model.pth")

# Generate text
model.eval()
context = "The quick brown fox"
encoded = text_to_token_ids(context, tokenizer)
encoded_tensor = encoded.to(device)

with torch.no_grad():
    token_ids = generate(
        model=model,
        idx=encoded_tensor,
        max_new_tokens=50,
        context_size=GPT_CONFIG_124M["context_length"],
        temperature=0.7,
        top_k=25
    )

decoded_text = token_ids_to_text(token_ids, tokenizer)
print(decoded_text)
```

---

## Key Training Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MONITORING TRAINING HEALTH                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Good Training Signs:
  âœ“ Train loss steadily decreasing
  âœ“ Val loss following train loss (not diverging)
  âœ“ Generated text improving over epochs
  âœ“ Loss not fluctuating wildly

Warning Signs:
  âœ— Val loss increasing while train loss decreasing â†’ Overfitting
  âœ— Both losses stuck at high value â†’ Learning rate too low
  âœ— Loss becomes NaN or inf â†’ Learning rate too high or numerical instability
  âœ— Val loss much higher than train loss â†’ Dataset too small

Typical Loss Values:
  Random initialization:  ~10.5  (log(50257) â‰ˆ 10.82)
  After 1 epoch:          ~6-8
  After 10 epochs:        ~4-6
  Well-trained (small):   ~3-4
  GPT-2 (full pretraining): ~2-3
```

---

## Optimization Details

**AdamW Optimizer:**
- Adaptive learning rates per parameter
- Momentum for smoother updates
- Weight decay for regularization
- Default betas: (0.9, 0.999)
- Epsilon: 1e-8

**Learning Rate:**
- Typical range: 1e-4 to 5e-4
- Too high: Training unstable, loss spikes
- Too low: Training too slow, may not converge

**Weight Decay:**
- L2 regularization on weights
- Typical: 0.1
- Prevents overfitting

**Batch Size:**
- Smaller batch (2-8): More gradient noise, slower but generalizes better
- Larger batch (32-128): Faster training, less noise, may overfit

---

## Code Location

- **Main notebook**: `ch05/01_main-chapter-code/ch05.ipynb`
- **Training script**: `ch05/01_main-chapter-code/gpt_train.py`
- **Generation script**: `ch05/01_main-chapter-code/gpt_generate.py`
- **Weight download**: `ch05/01_main-chapter-code/gpt_download.py`
- **Previous chapters**: `ch05/01_main-chapter-code/previous_chapters.py`

---

## Next Steps

After completing Chapter 5, you'll have:
- âœ… Trained a GPT model from scratch
- âœ… Implemented training and evaluation loops
- âœ… Loaded pretrained GPT-2 weights
- âœ… Generated text with temperature and top-k sampling
- âœ… Monitored training progress with loss curves

**Ready for Chapter 6**: Finetuning for classification! ğŸš€
